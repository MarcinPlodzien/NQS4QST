#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Dec 3 2025

@author: Marcin Plodzien 

 
===============================================================================
Neural Quantum Operator (NQO) Purification Scanner
===============================================================================


-------------------------------------------------------------------------------
OVERVIEW
-------------------------------------------------------------------------------
This script implements a *Neural Quantum Operator (NQO) Purification Ansatz* 
for data-driven quantum state tomography and entanglement certification.
The approach represents a general mixed quantum state rho as

    rho = A @ A_dagger ,

where A is a rectangular matrix of size (2^N, D) generated by a neural network.
Each column of A represents a "purification component" or virtual ancilla.
The latent dimension D sets the maximum rank of the reconstructed density matrix.

-------------------------------------------------------------------------------
PHYSICS CONTEXT
-------------------------------------------------------------------------------
1. Purification Ansatz
----------------------
In the purification framework, every mixed state rho acting on a physical 
Hilbert space H_S can be expressed as the partial trace of a pure state 
|Psi> defined on an extended space H_S x H_A (system plus ancilla):

    rho = Tr_ancilla[ |Psi><Psi| ] = A @ A_dagger .

Here:
- N = number of physical qubits (dimensionality 2^N).
- D = latent dimension = dimension of the ancilla subsystem.
  * D = 1  --> pure state representation.
  * D > 1  --> mixed state representation of rank <= D.

Each row of A corresponds to a computational basis configuration of the system,
and each column represents one ancilla degree of freedom.

2. Neural Parameterization
--------------------------
The neural network maps a configuration vector s (binary spin string) 
to a complex-valued D-dimensional output:

    Net(s) -> A(s) = [A_{s,1}, A_{s,2}, ..., A_{s,D}].

The probability of observing configuration s in the computational basis is

    P(s) = <s|rho|s> = sum_k |A_{s,k}|^2 = || A(s) ||^2.

Each amplitude A_{s,k} is generated via two real-valued Restricted Boltzmann 
Machines (RBMs) that separately parameterize the log-amplitude and the phase:

    A_{s,k} = exp[ A_theta(s) + i * Phi_theta(s) ] ,

where
    A_theta(s)  = sum_i a_i s_i + sum_j log(2 cosh(b_j + sum_i W_ij s_i)),
    Phi_theta(s) = sum_i c_i s_i + sum_j f(d_j + sum_i U_ij s_i),
    f(x) = (2/pi) * arctan(tanh(x)) + 0.5.

This amplitude–phase separation provides stable gradients for JAX autodiff.

3. Measurement Rotations  
---------------------------------------------
Computing probabilities in rotated measurement bases is achieved without 
explicitly rotating the full density matrix. Instead, we rotate each column 
of A by local unitaries:

    P(s | B) = || U_B @ A ||^2 ,

where U_B = tensor product of local basis transformations (I, H, or HS_dagger)
corresponding to Z, X, and Y measurement bases. 
Rotations are implemented via `jax.tensordot` loops rather than 
full 2^N × 2^N matrices, ensuring scalability.

-------------------------------------------------------------------------------
MATHEMATICAL FORMULATION
-------------------------------------------------------------------------------
1. Neural Operator Construction
-------------------------------
Given visible spins s_i in {-1,+1} and hidden spins h_j in {-1,+1}:
   E_theta(s, h) = -sum_i a_i s_i - sum_j b_j h_j - sum_{i,j} s_i W_ij h_j.

After summing over hidden variables:
   Psi_tilde_theta(s) = exp(sum_i a_i s_i) * product_j 2 cosh(b_j + sum_i W_ij s_i).

For the purification ansatz, each latent component l=1..D has independent 
parameters {a_l, b_l, W_l, c_l, d_l, U_l}. The full operator A is constructed as
   A_{s,l} = exp(A_theta_l(s) + i * Phi_theta_l(s)).

2. Mixed-State Density Matrix
-----------------------------
   rho = A @ A_dagger  = sum_l |psi_l><psi_l|,
where each psi_l = column_l(A) is a normalized neural wavefunction.
This ensures rho is Hermitian, positive semidefinite, and of rank <= D.

3. Measurement Probability
--------------------------
For a measurement basis B:
   p_theta(s | B) = <s| U_B rho U_B_dagger |s>
                  = sum_l | <s| U_B |psi_l> |^2.

The likelihood-based loss for tomography is
   L_NLL(theta) = -sum_{B,s} f_B(s) * log( p_theta(s|B) ),
where f_B(s) are empirical bitstring frequencies.

 
Physical constraints:
---------------------
- Each purification operator A normalized so that Tr(rho) = 1.
- rho guaranteed positive semidefinite by construction.
- Latent dimension D controls the expressiveness (rank) of the ansatz.

Entanglement Certification:
----------------------------
The code supports unconstrained (global) and structured (partitioned) 
purification architectures. Masking visible–hidden connectivity according 
to subsystem partitions enforces k-separability, allowing direct 
comparison of negative log-likelihood (NLL) gaps as entanglement witnesses.

-------------------------------------------------------------------------------
CONFIGURATION AND EXPERIMENTS
-------------------------------------------------------------------------------
- CONFIG and CONFIG_LIST define multiple simulation scenarios:
  * Rank_Collapse_Test (D=1): checks that pure states cannot fit noisy data.
  * Rank_Success_Test (D>1): verifies successful mixed-state reconstruction.
  * Phase_Blindness and Phase_Detection: test single- vs multi-basis data.
  * Entanglement_Death: simulates amplitude damping and decoherence.
  * Random_Local_Bases: full random local Pauli tomography.

Each scenario includes a set of ansatz architectures (unconstrained, 
bipartite, and fully separable) for entanglement-depth certification.

===============================================================================


"""

import os
import copy
import jax
import jax.numpy as jnp
from jax import random, tree_util, value_and_grad, lax, vmap, jit
from typing import Dict, List, Tuple, Optional
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
import optax  # Standard JAX optimizer

# Enable 64-bit precision for complex algebra stability
jax.config.update("jax_enable_x64", True)

FIGURES_FIG = "./figures_mixed_states_NQS_purification/"
if not os.path.exists(FIGURES_FIG):
    os.makedirs(FIGURES_FIG)
MEAS_DIR = "./measurement_data_mixed_states_NQS_purification"
if not os.path.exists(MEAS_DIR):
    os.makedirs(MEAS_DIR)

# =====================================================================
# 1. CONFIGURATION
# =====================================================================

CONFIG = {
    # --------------------------------------------------------------
    # Global settings
    # --------------------------------------------------------------
    "seed": 42,
    "N": 3,  # Number of Qubits

    # --------------------------------------------------------------
    # Measurement Strategy
    # --------------------------------------------------------------
    "loss_mode": "measurements", # NLL based on bitstring stats
    
    # Fixed bases (used if random_bases_per_shot is False)
    "measurement_bases": ["X", "Y", "Z"],

    # Finite Sampling simulation
    "shots": 1000,
    "shots_per_basis": 1000,

    # Randomized Tomography (Random Basis Set)
    "random_bases_per_shot": False,
    "num_random_bases": 27, 

    # --------------------------------------------------------------
    # Noise Model
    # --------------------------------------------------------------
    "noise_model": "local_dephasing",
    "p_noise": 0.05,

    # --------------------------------------------------------------
    # Neural Network (Purification Ansatz)
    # --------------------------------------------------------------
    # "latent_dim": The dimension of the Ancilla (Rank of approximation).
    # D=1 => Pure State. D=Rank(rho) => Exact Mixed State.
    "latent_dim": 5, 

    "H_amp": 24,    # Hidden units (Amplitude)
    "H_phase": 24,  # Hidden units (Phase)
    "H_amp_per": 12,    
    "H_phase_per": 12,

    # --------------------------------------------------------------
    # Training Loop
    # --------------------------------------------------------------
    "epochs": 2000,
    "lr": 0.02,
    "log_every": 10,

    # --------------------------------------------------------------
    # Targets
    # --------------------------------------------------------------
    "target_cases": [
        {"name": "GHZ State", "kind": "GHZ"},
        {"name": "W State",   "kind": "W"},
    ],

    # --------------------------------------------------------------
    # Ansatz Architectures (Entanglement Certification)
    # --------------------------------------------------------------
    "ansatzes": [
        {
            "name": "Unconstrained",
            "type": "Purification_Global",
            "partition_mode": "full", 
            "partitions": None   
        },
        {
            "name": "Bi Separable 0|12",
            "type": "Purification_Structured",
            "partition_mode": "explicit",
            "partitions": [[0], [1, 2]]
        },
        {
            "name": "Fully Separable",
            "type": "Purification_Structured",
            "partition_mode": "explicit",
            "partitions": [[i] for i in range(3)] 
        },
    ],
}

# =====================================================================
# CONFIG LIST (SCENARIOS ADAPTED FOR PURIFICATION)
# =====================================================================
CONFIG_LIST: List[Dict] = []

# SCENARIO 1: "Rank Capacity" Test
cfg_rank1 = copy.deepcopy(CONFIG)
cfg_rank1["name"] = "Rank_Collapse_Test_Lat=1"
cfg_rank1["noise_model"] = "depolarizing"
cfg_rank1["p_noise"] = 0.15
cfg_rank1["latent_dim"] = 1  # Forces Pure State
CONFIG_LIST.append(cfg_rank1)

cfg_rank5 = copy.deepcopy(CONFIG)
cfg_rank5["name"] = "Rank_Success_Test_Lat=5"
cfg_rank5["noise_model"] = "depolarizing"
cfg_rank5["p_noise"] = 0.15
cfg_rank5["latent_dim"] = 5  # Allows Mixed State
CONFIG_LIST.append(cfg_rank5)

# SCENARIO 2: "Phase Blindness" vs "Phase Detection"
cfg_blind = copy.deepcopy(CONFIG)
cfg_blind["name"] = "Phase_Blindness_Z_Only"
cfg_blind["noise_model"] = "local_dephasing"
cfg_blind["p_noise"] = 0.02
cfg_blind["measurement_bases"] = ["Z"]
cfg_blind["latent_dim"] = 3
CONFIG_LIST.append(cfg_blind)

cfg_sight = copy.deepcopy(CONFIG)
cfg_sight["name"] = "Phase_Detection_XYZ"
cfg_sight["noise_model"] = "local_dephasing"
cfg_sight["p_noise"] = 0.05
cfg_sight["measurement_bases"] = ["X", "Y", "Z"]
cfg_sight["latent_dim"] = 3
CONFIG_LIST.append(cfg_sight)

# SCENARIO 3: "Entanglement Sudden Death"
cfg_death = copy.deepcopy(CONFIG)
cfg_death["name"] = "Entanglement_Death_High_T1"
cfg_death["noise_model"] = "amplitude_damping"
cfg_death["p_noise"] = 0.05
cfg_death["measurement_bases"] = ["X", "Y", "Z"]
cfg_death["latent_dim"] = 3
CONFIG_LIST.append(cfg_death)

# SCENARIO 4: NISQ-like finite shots
cfg_nisq = copy.deepcopy(CONFIG)
cfg_nisq["name"] = "NISQ_Simulation_1k_Shots"
cfg_nisq["noise_model"] = "depolarizing"
cfg_nisq["p_noise"] = 0.02
cfg_nisq["shots"] = 1000
cfg_nisq["shots_per_basis"] = 1000
cfg_nisq["measurement_bases"] = ["X", "Y", "Z"]
cfg_nisq["latent_dim"] = 3
CONFIG_LIST.append(cfg_nisq)

# SCENARIO 5: Random local XYZ bases
cfg_rand_xyz = copy.deepcopy(CONFIG)
cfg_rand_xyz["name"] = "Random_Local_XYZ_Bases"
cfg_rand_xyz["noise_model"] = "depolarizing"
cfg_rand_xyz["p_noise"] = 0.05
cfg_rand_xyz["measurement_bases"] = ["X", "Y", "Z"]
cfg_rand_xyz["shots"] = 1000
cfg_rand_xyz["shots_per_basis"] = 1000
cfg_rand_xyz["random_bases_per_shot"] = True
cfg_rand_xyz["num_random_bases"] = 3 ** cfg_rand_xyz["N"]
cfg_rand_xyz["latent_dim"] = 3
CONFIG_LIST.append(cfg_rand_xyz)


# =====================================================================
# 2. UTILITIES: OPTIMIZED MATH (VMAP/JIT)
# =====================================================================

def bin_to_spin(bin_array: jnp.ndarray) -> jnp.ndarray:
    return 2 * bin_array - 1

def get_all_configs(N: int) -> jnp.ndarray:
    ints = jnp.arange(2**N, dtype=jnp.int32)
    bits = (ints[:, None] >> jnp.arange(N)) & 1
    return bin_to_spin(bits)

# --- Rotation Logic (No Global Matrices) ---

# Shape (3, 2, 2): [Identity, Hadamard, Y-basis]
LOCAL_BASIS_MATS = jnp.stack([
    jnp.eye(2, dtype=jnp.complex128),
    (1.0 / jnp.sqrt(2.0)) * jnp.array([[1.0, 1.0], [1.0, -1.0]], dtype=jnp.complex128),
    (1.0 / jnp.sqrt(2.0)) * jnp.array([[1.0, -1j], [1.0, 1j]], dtype=jnp.complex128),
], axis=0)

AXIS_TO_INDEX = {"Z": 0, "X": 1, "Y": 2}

@jit
def apply_single_basis_rotation(psi_flat: jnp.ndarray, basis_axes: jnp.ndarray) -> jnp.ndarray:
    """
    Rotate a state vector |psi> (2^N) by local unitaries.
    Uses tensordot to avoid creating 2^N x 2^N matrices.
    """
    N = basis_axes.shape[0]
    psi = psi_flat.reshape((2,) * N)
    
    for q in range(N):
        axis_idx = basis_axes[q]
        U = LOCAL_BASIS_MATS[axis_idx]
        # Contract axis q
        psi = jnp.tensordot(psi, U, axes=[[q], [1]])
        # Permute back
        perm = list(range(N-1))
        perm.insert(q, N-1)
        psi = jnp.transpose(psi, perm)

    return psi.reshape(-1)

# VMAP: Rotate (2^N) state against (B) bases -> (B, 2^N)
batch_apply_rotations = vmap(apply_single_basis_rotation, in_axes=(None, 0))

def encode_basis_specs_to_axes(basis_specs: List[str], N: int) -> jnp.ndarray:
    rows = []
    for spec in basis_specs:
        if len(spec) == 1: spec = spec * N
        indices = [AXIS_TO_INDEX[c.upper()] for c in spec]
        rows.append(indices)
    return jnp.array(rows, dtype=jnp.int32)


# =====================================================================
# 3. NOISE MODELS & TARGET RHO
# =====================================================================

def apply_kraus_map(rho: jnp.ndarray, kraus_ops: List[jnp.ndarray]) -> jnp.ndarray:
    rho_new = jnp.zeros_like(rho)
    for K in kraus_ops:
        rho_new += K @ rho @ K.conj().T
    return rho_new

def get_local_kraus_channel(rho: jnp.ndarray, kraus_single: List[jnp.ndarray]) -> jnp.ndarray:
    d = rho.shape[0]
    N = int(np.round(np.log2(d)))
    current_rho = rho
    for q in range(N):
        # Build K_full = I x ... x K_loc x ... x I
        full_ops = []
        for K_loc in kraus_single:
            lst = [jnp.eye(2, dtype=jnp.complex128)] * N
            lst[q] = K_loc
            K_f = lst[0]
            for i in range(1, N): K_f = jnp.kron(K_f, lst[i])
            full_ops.append(K_f)
        current_rho = apply_kraus_map(current_rho, full_ops)
    return current_rho

def apply_noise_channel(rho: jnp.ndarray, p: float, channel: str) -> jnp.ndarray:
    d = rho.shape[0]
    if channel == "depolarizing":
        return (1 - p) * rho + p * jnp.eye(d) / d
    elif channel == "local_dephasing":
        indices = jnp.arange(d, dtype=jnp.int32)
        dist = jax.lax.population_count(jnp.bitwise_xor(indices[:, None], indices[None, :]))
        return rho * jnp.power(1.0 - p, dist)
    elif channel == "amplitude_damping":
        K0 = jnp.array([[1.0, 0.0], [0.0, jnp.sqrt(1 - p)]], dtype=jnp.complex128)
        K1 = jnp.array([[0.0, jnp.sqrt(p)], [0.0, 0.0]], dtype=jnp.complex128)
        return get_local_kraus_channel(rho, [K0, K1])
    elif channel == "bit_flip":
        I = jnp.eye(2, dtype=jnp.complex128)
        X = jnp.array([[0.0, 1.0], [1.0, 0.0]], dtype=jnp.complex128)
        K0 = jnp.sqrt(1 - p) * I
        K1 = jnp.sqrt(p) * X
        return get_local_kraus_channel(rho, [K0, K1])
    elif channel == "thermal":
        pop_ground = 0.8
        sqrt_pop = jnp.sqrt(pop_ground); sqrt_exc = jnp.sqrt(1 - pop_ground)
        sqrt_1_p = jnp.sqrt(1 - p); sqrt_p = jnp.sqrt(p)
        K0 = sqrt_pop * jnp.array([[1.0, 0.0], [0.0, sqrt_1_p]], dtype=jnp.complex128)
        K1 = sqrt_pop * jnp.array([[0.0, sqrt_p], [0.0, 0.0]], dtype=jnp.complex128)
        K2 = sqrt_exc * jnp.array([[sqrt_1_p, 0.0], [0.0, 1.0]], dtype=jnp.complex128)
        K3 = sqrt_exc * jnp.array([[0.0, 0.0], [sqrt_p, 0.0]], dtype=jnp.complex128)
        return get_local_kraus_channel(rho, [K0, K1, K2, K3])
    return rho

def get_ghz(N):
    psi = jnp.zeros(2**N, dtype=complex); psi=psi.at[0].set(1.); psi=psi.at[-1].set(1.)
    return psi/jnp.linalg.norm(psi)

def get_w(N):
    psi = jnp.zeros(2**N, dtype=complex)
    for i in range(N): psi=psi.at[1<<i].set(1.)
    return psi/jnp.linalg.norm(psi)

def build_target_rho(N, case_cfg, noise, p):
    if case_cfg["kind"] == "GHZ": psi = get_ghz(N)
    elif case_cfg["kind"] == "W": psi = get_w(N)
    else: psi = get_ghz(N)
    rho = jnp.outer(psi, psi.conj())
    return apply_noise_channel(rho, p, noise)


# =====================================================================
# 3b. DATA GENERATION (Vectorized)
# =====================================================================

def make_data_probs(rho: jnp.ndarray,
                    N: int,
                    basis_labels: List[str],
                    shots: int,
                    seed: int,
                    random_bases: bool = False,
                    num_random: int = 27) -> Tuple[jnp.ndarray, List[str], List[Dict]]:
    
    rng = np.random.RandomState(seed)
    dim = 2**N
    measurements = []

    # 1. Generate Basis Specs
    if random_bases:
        allowed = [b.upper() for b in basis_labels]
        pool = []
        seen = set()
        limit = min(num_random, len(allowed)**N)
        while len(pool) < limit:
            s = "".join([rng.choice(allowed) for _ in range(N)])
            if s not in seen:
                seen.add(s); pool.append(s)
        basis_specs = pool
    else:
        basis_specs = [b*N if len(b)==1 else b for b in basis_labels]

    # 2. Encode to Axes
    basis_axes = encode_basis_specs_to_axes(basis_specs, N)

    # 3. Compute Probabilities (Vectorized)
    # rho = sum_j lam_j |j><j|
    evals, evecs = jnp.linalg.eigh(rho)
    evals = jnp.clip(evals, 0, None); evals/=evals.sum()
    psi_j = evecs.T 

    # Vmap rotation over eigenvectors
    # Inner: rotate one state by all bases
    def rotate_all(psi): return batch_apply_rotations(psi, basis_axes)
    
    psi_rot = vmap(rotate_all)(psi_j) # (dim_j, n_bases, 2^N)
    probs_j = jnp.abs(psi_rot)**2
    
    # Weighted sum
    probs_exact = jnp.einsum('j, jbs -> bs', evals, probs_j)
    probs_exact = probs_exact / jnp.sum(probs_exact, axis=1, keepdims=True)

    # 4. Sampling
    probs_list = []
    probs_np = np.array(probs_exact)
    
    for i, spec in enumerate(basis_specs):
        p = probs_np[i]
        if shots > 0:
            s = rng.choice(dim, size=shots, p=p)
            counts = np.bincount(s, minlength=dim)/shots
            probs_list.append(jnp.array(counts))
            for outcome in s:
                measurements.append({"outcome_int": int(outcome), "basis_spec": spec})
        else:
            probs_list.append(jnp.array(p))
            
    return jnp.stack(probs_list), basis_specs, measurements


# =====================================================================
# 4. NEURAL PURIFICATION MODELS
# =====================================================================

def init_purification_params(key, N, latent_dim, Ha, Hp, scale=0.01):
    keys = random.split(key, 6)
    return {
        "a": scale*random.normal(keys[0],(latent_dim, N)), 
        "b": scale*random.normal(keys[1],(latent_dim, Ha)),
        "W": scale*random.normal(keys[2],(latent_dim, N, Ha)), 
        "c": scale*random.normal(keys[3],(latent_dim, N)),
        "d": scale*random.normal(keys[4],(latent_dim, Hp)), 
        "U": scale*random.normal(keys[5],(latent_dim, N, Hp))
    }

def init_ansatz_params(key, N, partitions, cfg):
    if partitions is None:
        return init_purification_params(key, N, cfg["latent_dim"], cfg["H_amp"], cfg["H_phase"])
    else:
        keys = random.split(key, len(partitions))
        params_list = []
        for k, part in zip(keys, partitions):
            sub_N = len(part)
            params_list.append(init_purification_params(k, sub_N, cfg["latent_dim"], cfg["H_amp_per"], cfg["H_phase_per"]))
        return params_list

def purification_forward_single(p, s):
    # Amplitude
    bias_vis = jnp.einsum("bi,li->bl", s, p["a"])
    W_term = jnp.einsum("bi,lih->blh", s, p["W"]) 
    hidden_term = p["b"][None, :, :] + W_term
    ln_cosh = jnp.sum(jnp.log(2.0*jnp.cosh(hidden_term)), axis=-1)
    log_amp = bias_vis + ln_cosh
    amp = jnp.exp(log_amp)
    
    # Phase
    bias_vis_ph = jnp.einsum("bi,li->bl", s, p["c"])
    W_term_ph = jnp.einsum("bi,lih->blh", s, p["U"]) 
    hidden_term_ph = p["d"][None, :, :] + W_term_ph
    phase_hidden = jnp.sum((2/jnp.pi)*jnp.arctan(jnp.tanh(hidden_term_ph)), axis=-1)
    phase = bias_vis_ph + phase_hidden
    
    return amp * jnp.exp(2j * jnp.pi * phase)

def purification_forward_structured(params, partitions, s):
    if partitions is None:
        return purification_forward_single(params, s)
    
    A_blocks = []
    for idx, part in enumerate(partitions):
        s_part = s[:, jnp.array(part)]
        A_blocks.append(purification_forward_single(params[idx], s_part))
        
    A_total = A_blocks[0]
    for i in range(1, len(A_blocks)):
        prev = A_total[:, :, None]
        next_ = A_blocks[i][:, None, :]
        prod = prev * next_
        B = prod.shape[0]
        A_total = prod.reshape(B, -1)
        
    return A_total


# =====================================================================
# 5. METRICS & TRAINING
# =====================================================================

@jit
def get_probs_vectorized(params, partitions, all_s, basis_axes):
    # 1. Get Global Purification A(s) (Batch_s, Latent_Global)
    A_s = purification_forward_structured(params, partitions, all_s)
    
    # Normalize globally: sum_s sum_k |A_sk|^2 = 1
    norm = jnp.sqrt(jnp.sum(jnp.abs(A_s)**2))
    A_s = A_s / norm

    # 2. Rotate A
    A_T = A_s.T # (Latent, 2^N)
    
    # vmap over latent dim D
    def rotate_latent_vector(psi): return batch_apply_rotations(psi, basis_axes)
    
    # Result: (Latent, Bases, 2^N)
    A_rotated = vmap(rotate_latent_vector)(A_T)
    
    # 3. Probabilities: Sum over Latent dimension
    # P(s|b) = sum_d |(U_b A)_ds|^2
    A_final = jnp.transpose(A_rotated, (1, 2, 0)) # (Bases, 2^N, Latent)
    probs = jnp.sum(jnp.abs(A_final)**2, axis=-1) # (Bases, 2^N)
    
    return probs

@jit
def get_rho_reconstructed(params, partitions, all_s):
    A = purification_forward_structured(params, partitions, all_s)
    norm = jnp.sqrt(jnp.sum(jnp.abs(A)**2))
    A = A / norm
    return A @ A.conj().T

@jit
def nll_loss(params, partitions, data_probs, all_s, basis_axes):
    p_model = get_probs_vectorized(params, partitions, all_s, basis_axes)
    # Avoid log(0)
    return -jnp.mean(jnp.sum(data_probs * jnp.log(p_model + 1e-12), axis=1))

def compute_detailed_metrics(rho_est, rho_target):
    p_est = jnp.real(jnp.trace(rho_est @ rho_est))
    p_tgt = jnp.real(jnp.trace(rho_target @ rho_target))
    ov = jnp.real(jnp.trace(rho_est @ rho_target))
    fid = ov / (jnp.sqrt(p_est * p_tgt) + 1e-12)
    return fid, p_est, p_tgt

def train(key, N, partitions, cfg, data_probs, basis_axes, all_s, rho_target):
    params = init_ansatz_params(key, N, partitions, cfg)
    
    optimizer = optax.adam(learning_rate=cfg["lr"])
    opt_state = optimizer.init(params)
    
    @jit
    def update_step(params, opt_state):
        loss, grads = value_and_grad(nll_loss)(params, partitions, data_probs, all_s, basis_axes)
        updates, opt_state = optimizer.update(grads, opt_state, params)
        params = optax.apply_updates(params, updates)
        return params, opt_state, loss

    steps, loss_h, fid_h, pur_h = [], [], [], []
    print(f"\n>> Training: Latent={cfg['latent_dim']} | Noise={cfg['noise_model']}")
    print(f"{'Ep':<5} | {'Loss':<7} | {'Fidelity':<7} | {'Purity':<7}")
    print("-" * 35)

    for i in range(1, cfg["epochs"] + 1):
        params, opt_state, loss_val = update_step(params, opt_state)
        
        if i % cfg["log_every"] == 0:
            rho_est = get_rho_reconstructed(params, partitions, all_s)
            fid, p_est, p_tgt = compute_detailed_metrics(rho_est, rho_target)
            print(f"{i:<5d} | {loss_val:<7.4f} | {float(fid):<7.4f} | {float(p_est):.3f}")
            steps.append(i); loss_h.append(float(loss_val)); fid_h.append(float(fid)); pur_h.append(float(p_est))
            
    rho_est = get_rho_reconstructed(params, partitions, all_s)
    finals = compute_detailed_metrics(rho_est, rho_target)
    return steps, loss_h, fid_h, pur_h, finals


# =====================================================================
# 6. IO & VISUALIZATION (STRICT PARITY WITH ENSEMBLE CODE)
# =====================================================================

def config_suffix(cfg: Dict) -> str:
    # Bases tag
    bases_list = cfg.get("measurement_bases", [])
    if cfg.get("random_bases_per_shot", False):
        bases_tag = "rand-" + "".join(str(b) for b in bases_list)
    else:
        bases_tag = "".join(str(b) for b in bases_list)

    # Shots tag
    shots_pb = cfg.get("shots", 0)
    shots_tag = "inf" if shots_pb is None or shots_pb <= 0 else str(shots_pb)

    # Effective bases (nbases)
    nbases = cfg.get("nbases_effective", None)
    nbases_tag = f"_nbases-{nbases}" if nbases is not None else ""

    # Nominal shots per basis (shpb)
    shpb_nom = cfg.get("shots_per_basis_nominal", None)
    if shpb_nom is None:
        shpb_nom = "inf" if shots_pb is None or shots_pb <= 0 else shots_pb
    shpb_tag = f"_shpb-{shpb_nom}"

    # Noise and Latent Dimension
    noise_tag = f"_noise-{cfg['noise_model']}_p{cfg['p_noise']}"
    lat_tag = f"_Lat-{cfg['latent_dim']}"

    return f"N{cfg['N']}_bases-{bases_tag}_shots-{shots_tag}{nbases_tag}{shpb_tag}{noise_tag}{lat_tag}"

def target_case_tag(case_cfg):
    kind = case_cfg["kind"]
    if kind == "GHZ": return "GHZ"
    elif kind == "W": return "W"
    else: return kind

def save_measurement_data(N, measurements, case_cfg, cfg, out_dir=MEAS_DIR):
    if not measurements: return
    filename = f"measdata_purification_{target_case_tag(case_cfg)}_{config_suffix(cfg)}.txt"
    path = os.path.join(out_dir, filename)
    with open(path, "w") as f:
        f.write("bitstring," + ",".join(f"b{i}" for i in range(N)) + "\n")
        for rec in measurements:
            bits = format(rec["outcome_int"], f"0{N}b")
            spec = rec["basis_spec"]
            if len(spec) == 1: spec = spec * N
            f.write(",".join([bits] + list(spec)) + "\n")
    print(f"    Saved measurement data to {path}")

def resolve_partitions(N, ans_cfg):
    if ans_cfg["partition_mode"] == "full": return None
    if ans_cfg["partition_mode"] == "explicit": return ans_cfg["partitions"]
    return [[i] for i in range(N)] 

def plot_results_grid(results, cfg):
    n_cases = len(results)
    # 3 columns: Loss, Fidelity, Purity
    fig, axes = plt.subplots(n_cases, 3, figsize=(15, 4*n_cases), squeeze=False)
    suffix = config_suffix(cfg)
    

    title_meta = (
        f"Noise: {cfg['noise_model']} (p={cfg['p_noise']}) | "
        f"Bases: {cfg['measurement_bases']} | "
        f"Shots_per_basis: {cfg['shots']} | "
        f"Latent Dim={cfg['latent_dim']}"
    )
    
    for i, (case, ans_dict) in enumerate(results):
        axl, axf, axp = axes[i]
        for name, data in ans_dict.items():
            steps, loss, fid, pur, finals = data
            label = f"{name} (F={finals[0]:.3f})"
            axl.plot(steps, loss, label=label, lw=2)
            axf.plot(steps, fid, label=label, lw=2)
            axp.plot(steps, pur, label=label, lw=2)
            p_tgt = finals[2]
            
        axp.axhline(float(p_tgt), c='k', ls='--', label="Target Purity")
        
        axl.set_title(f"{case}\nNLL Loss")
        axf.set_title("HS Fidelity")
        axp.set_title("Purity Tr[rho^2]")
        axf.set_ylim(-0.05, 1.05)
        axp.set_ylim(-0.05, 1.05)
        
        if i==0: 
            axl.legend(fontsize=8)
            axp.legend(fontsize=8)
            
    plt.suptitle(title_meta)
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    
    filename = FIGURES_FIG + f"Purification_Grid_{suffix}.png"
    print(filename)
    plt.savefig(filename, dpi=150)
    plt.show()

def run_experiment(cfg):
    print(f"\n=== EXPERIMENT: {cfg['name']} ===")
    
    s = cfg.get("shots_per_basis", 0)
    if s == 0: s = cfg.get("shots", 0)
    cfg["shots"] = s
    cfg["shots_per_basis"] = s
    
    key = random.PRNGKey(cfg["seed"])
    N = cfg["N"]; all_s = get_all_configs(N)
    results = []
    
    for case in cfg["target_cases"]:
        print(f"\nTarget: {case['name']}")
        rho_t = build_target_rho(N, case, cfg["noise_model"], cfg["p_noise"])
        
        # Data Generation (Vectorized)
        d_prob, basis_specs, meas = make_data_probs(
            rho_t, N, cfg["measurement_bases"], cfg["shots"], cfg["seed"],
            random_bases=cfg.get("random_bases_per_shot", False),
            num_random=cfg.get("num_random_bases", 27)
        )
        
        cfg["nbases_effective"] = len(basis_specs)
        cfg["shots_per_basis_nominal"] = cfg["shots"] if cfg["shots"] > 0 else "inf"
        
        save_measurement_data(N, meas, case, cfg)
        basis_axes = encode_basis_specs_to_axes(basis_specs, N)
        
        ans_res = {}
        for ans in cfg["ansatzes"]:
            print(f"  > Ansatz: {ans['name']}")
            parts = resolve_partitions(N, ans)
            k_tr, k_init = random.split(key)
            ans_res[ans['name']] = train(k_tr, N, parts, cfg, d_prob, basis_axes, all_s, rho_t)
        results.append((case["name"], ans_res))
        
    plot_results_grid(results, cfg)

def main():
    cfgs = CONFIG_LIST if CONFIG_LIST else [CONFIG]
    for cfg in cfgs: run_experiment(cfg)

if __name__ == "__main__":
    main()
